---

title: "Week 3"
excerpt: ""

---

This week I have started another exercise from Robotics-Academy: [Cat And Mouse Exercise](http://jderobot.github.io/RoboticsAcademy/exercises/Drones/drone_cat_mouse). This exercise involves **2 drones**: one acts as the **mouse** (the red one) and the other acts as the **cat** (the black one). 

![image](../assets/images/cat_and_mouse.png)

The mouse moves randomly and the goal of this exercise is to program the logic/intelligence of the cat: the cat has to search, chase and stay close to the mouse. 

There are several methods accesible through the Hardware Abstraction Layer (HAL) which act as sensors and return information such as the actual position, velocities and angles(yaw, roll, pitch) of the drone. Just as a reminder, the different angles mencioned are represented in the next image:

![image](../assets/images/Flight_dynamics_with_text.png)

The methods that allow to control the drone:
- Position control --> **HAL.set_cmd_pos(x, y, z, az)**
- Velocity control --> **HAL.set_cmd_vel(vx, vy, vz, az)**
- Mixed control --> **HAL.set_cmd_mix(vx, vy, x, az)**

The input argument named 'az' represents the **yaw angle**, and it's the only angle that can be controlled. 

In addition, there are some other methods to get and show images from the two cameras of the drone (one frontal and the other one ventral): 
- **HAL.get_frontal_image()**
- **HAL.get_ventral_image()**
- **GUI.showImage(img)**
- **GUI.showLeftImage(img)**

The first step that must be done is to filter the image we get from the frontal camera of the drone. As the mouse is red, a red filter has been implemented. Besides, as a first approach to obtain the position of the mouse, the centroid of the filtered drone has been calculated, as can be seen in the next image:

![image](../assets/images/mouse_drone_filtered.png)


